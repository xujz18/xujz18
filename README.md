### Hi, welcome to my Github ðŸ‘‹

I am Jiazheng Xu, a second-year PhD student in Tsinghua University.

- ðŸ”­ Interested in Machine Learning, Multimodal Generative Models.
- ðŸŒ± Find my up-to-date publication list in [**Google Scholar**](https://scholar.google.com/citations?user=7--T2_4AAAAJ)! Some of my proud leading works:
  * [ImageReward (NeurIPS'23)](https://github.com/THUDM/ImageReward): the first general-purpose text-to-image human preference reward model (RM) for RLHF, outperforming CLIP/BLIP/Aesthetic by 30% in terms of human preference prediction.
  * [CogVLM](https://github.com/THUDM/CogVLM): a powerful open-source visual language model (VLM), which achieves state-of-the-art performance on 10 classic cross-modal benchmarks.
  * [CogAgent](https://github.com/THUDM/CogVLM): an open-source visual language model improved based on CogVLM, which possesses the capabilities of a visual Agent, being able to return a plan, next action, and specific operations with coordinates for any given task on any GUI screenshot, enhancing GUI-related question-answering capabilities.
- ðŸ’¬ Feel free to drop me an email for:
  * Any form of collaboration
  * Any issue about my works or code
  * Interesting ideas to discuss or just chatting
